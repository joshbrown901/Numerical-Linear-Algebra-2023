{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497e4167",
   "metadata": {
    "id": "qQ3x6VfoEpm9"
   },
   "source": [
    "# Problem set 2 (99 pts)\n",
    "\n",
    "## Important information\n",
    "\n",
    "1. We provide signatures of the functions that you have to implement. Make sure you follow the signatures defined, otherwise your coding solutions will not be graded.\n",
    "\n",
    "2. Please submit the single Jupyter Notebook file, where only Python and Markdown/$\\LaTeX$\n",
    " are used. Any hand-written solutions inserted by photos or in any other way are prohibitive and will not be graded. If you will have any questions about using Markdown, ask them!\n",
    "\n",
    "3. The works will be checked for plagiarism. The score will be divided by the number of similar works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8a6cc",
   "metadata": {},
   "source": [
    "# Problem 1 (44 pts)\n",
    "\n",
    "Commonly enough the matrices arising in discretisations of particular problems are not just sparse, but also possess the *banded* property i.e. a general element of $A$ be $a_{ij}$, we say that it has an *upper bandwith* of $u$ and a *lower bandwith* of $l$ iff it may only nonzero between $l$-th and $u$-th main diagonals (in other words, $a_{ij}$ may only be nonzero when $i\\le j+l$ and $j\\le i+ u$). \n",
    "In particular, any upper triangular matrix of size $n\\times n$ has a lower bandwith of 1 and an upper bandwith of $n$. \n",
    "The goal of this task is to endow you with some understanding of this important matrix class. Keep in mind that vectors are column.\n",
    "\n",
    "1) (1 pts) Assume you are given a dyadic matrix $xy^T$. Suppose that only $a$ leading components of $x$ and $b$ leading components of $y$ are nonzero. What are the bandwiths of this matrix?\n",
    "\n",
    "\n",
    "2) (3 pts) Assume you are given a banded matrix $A\\in\\mathbb{R}^{n\\times n}$ with bandwiths $l$ and $u$. Examine the bandwidths of its $L$ and $U$ factors in LU decomposition, assuming its existence. Assume that the diagonal is nonzero for simplicity of the proof.\n",
    "\n",
    "3) (3 pts) Assume we also perform pivoting in computing PLU decomposition, so that $A\\ne LU$, but rather $PA = LU$. Examine the upper bandwidth of $U$.\n",
    "\n",
    "\n",
    "4) (7 pts) Based on the intuition gained so far, propose and implement an LU algorithm for banded matrices specifically. Estimate the leading term in the complexity analysis. Pivoting is not needed.\n",
    "\n",
    "\n",
    "5) (5 pts) Specify the above algorithm to tridiagonal matrices. Implement it and estimate complexity.\n",
    "\n",
    "\n",
    "6) (10 pts) Since we are dealing with direct inversions, we would also like to know how the banded matrices invert. In particular, we would like to know if we can say something about inverses' structure. Show that the inverse of a banded matrix is a matrix with low-rank off-diagonal blocks and estimate the ranks thereof. Assume the matrix diagonal blocks nonsingular.\n",
    "\n",
    "7) (10 pts) Finally, proof the following important fact, due to Strang: if both a matrix and its inverse are banded, then the matrix is a product of block diagonal factors; each factor is composed of $2\\times2$ and $1\\times1$ blocks, and the total number thereof is not dependent directly on the matrix dimension. Try to make it as convincing as you can.\n",
    "\n",
    "8) (5 pts) Consider the inverse matrix of a banded matrix with the strcuture from task 7); consider also the transpose thereof. Show that they share bandwidths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d370d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37600aad",
   "metadata": {},
   "source": [
    "# Problem 2. Stability of linear least squares (25 pts)\n",
    "\n",
    "In this task you are supposed to explore the concept of the stability of the linear least squares problem.\n",
    "\n",
    "So you are given a matrix $A \\in \\mathbf{R}^{n \\times m}, n > m$, the right-hand side $b \\in \\mathbf{R}^{n}$. And you need to find the solution $x \\in \\mathbf{R}^{m}$ that minimizes the residual:\n",
    "\n",
    "$$\n",
    "\\Vert A x - b \\Vert_2 \\rightarrow \\min_x.\n",
    "$$\n",
    "\n",
    "### Task 1. Theory\n",
    "For all tasks here you can use spectral norm for the computation of condition number.\n",
    "\n",
    "1. (5 pts) Condition number of a matrix affects the stability of the solution. Recall from the lecture that using a normal equation $ A^* A x = A^* b $ to solve linear least squares problem is not a good idea.\n",
    "Prove that $\\mathrm{cond}_2(A^* A) = \\mathrm{cond}_2(A)^2$.\n",
    "\n",
    "2. (5 pts) Derive the condition number in 2-norm for\n",
    "  \n",
    "   a) unitary matrix,\n",
    "   \n",
    "   b) normal matrix,\n",
    "\n",
    "   c) diagonal matrix.\n",
    "\n",
    "### Task 2. Practice (15 pts)\n",
    "\n",
    "In this task you need to compare the relative errors of solution produced by different approaches for solving linear least squares on ill-conditioned matrices.\n",
    "\n",
    "1. Come up with the way to construct a random $n \\times m$ matrix with given condition number. Implement it in `construct_random_matrix`.\n",
    "\n",
    "2. You need to solve linear least squares problem using four ways:\n",
    "   \n",
    "   a) with normal equation;\n",
    "   \n",
    "   b) using QR decomposition;\n",
    "   \n",
    "   c) using pseudoinverse (compute it yourself using SVD);\n",
    "   \n",
    "   d) using bigger system of equations\n",
    "   \n",
    "   $$ \\begin{pmatrix} 0 & A^* \\\\ A & -I \\end{pmatrix} \\begin{pmatrix} x \\\\ r \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ b \\end{pmatrix}, $$\n",
    "   \n",
    "    where $r = Ax - b$.\n",
    "\n",
    "3. With the growth of condition number show the growth of the relative error $\\frac{\\Vert \\hat{x} - x \\Vert}{\\Vert x \\Vert}$ between the initial solution $x$ and the solution $\\hat{x}$ of the perturbed system $Ax = b + \\Delta b$. Show that for the normal equation the dependence is quadratic w.r.t condition number, while for the other methods it is linear. Plot the resulting dependence (error vs condition number) and comment on results. Ensure that your plots are interpretable.\n",
    "\n",
    "4. In general, the setup of your experiment should be the following:\n",
    " - Generate a random matrix $A$ and a vector $x_\\text{true}$.\n",
    " - Compute the correct right-hand side $b$.\n",
    " - Solve the resulting system $Ax = b$ to get $x$.\n",
    " - Perturb the system $Ax = b + \\Delta b$ and get new solution $\\hat{x}$. The perturbation $\\frac{\\Vert \\Delta b \\Vert}{\\Vert b \\Vert}$ should be small.\n",
    "\n",
    "You are free to use `numpy` or `jax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cddb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def construct_random_matrix(n: int, m: int, cond_number: float):\n",
    "    \"\"\"\n",
    "    Constructs random nxm matrix with a given condition matrix.\n",
    "\n",
    "    Args:\n",
    "        n, m: matrix dimensions\n",
    "        cond_number: the desired condition numebr\n",
    "\n",
    "    Returns: nxm matrix with condition number cond_number\n",
    "    \"\"\"\n",
    "    # Your solution here\n",
    "    pass\n",
    "\n",
    "\n",
    "n = 100\n",
    "m = 40\n",
    "cond_number = np.random.random() * 1000\n",
    "\n",
    "assert np.allclose(np.linalg.cond(construct_random_matrix(n, m, cond_number)), cond_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71aa55",
   "metadata": {},
   "source": [
    "# Problem 3 (30 pts)\n",
    "\n",
    "A generalisation of the eigenvalue problem is the generalised eigenvalue problem; if an eigenpair satisfies \n",
    "$$ Av = \\lambda v,$$\n",
    "then a generalised eigenvalue for a pair $(A,B)$ of matrices makes the matrix \n",
    "$$A-\\lambda B$$ \n",
    "degenerate.\n",
    "The generalised eigenvector satisfies $Av = \\lambda Bv$. In this task we consider matrices $A,B$ are of size  $n\\times n$.\n",
    "\n",
    "## Task 1 (4 pts)\n",
    "\n",
    "How many generalised eigenvalues are there for each of the following matrix pairs? Find all, if you can:\n",
    "\n",
    "  1. $$ A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 3 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 1 & 0 \\\\ 0& 1 \\end{bmatrix} $$ \n",
    "\n",
    "  2. $$ A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 3 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 1 & 0 \\\\ 0& 0 \\end{bmatrix} $$\n",
    "  \n",
    "  3. $$ A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 3 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 0 & 1 \\\\ 0& 0 \\end{bmatrix} $$\n",
    "  \n",
    "  4. $$ A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 0 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 1 & 0 \\\\ 0& 0 \\end{bmatrix} $$\n",
    "\n",
    "## Task 2 (2 pts)\n",
    "\n",
    "Assume $\\mu$ is not a generalised eigenvalue of $(A,B)$. How are spectra of $(A,B)$ and $((A-\\mu B)^{-1}A,(A-\\mu B)^{-1}B)$ related?\n",
    "\n",
    "## Task 3 (24 pts)\n",
    "\n",
    "To solve the complete generalised eigenvalue problem, [QZ algorithm](https://link.springer.com/content/pdf/10.1007/3-540-28502-4_2) can be used. This algorithm is the direct generalization of the QR algorithm for the standard eigenvalue problem. \n",
    "However, in this task we focus on the partial generalised eigenvalue problem and its application to the canonical correlation analysis.\n",
    "\n",
    "You can test the algorithms in the further tasks only for the symmetric matrices $A$ and symmetric positive definite matrices $B$.\n",
    "\n",
    "### From power method to its generalisation form\n",
    "\n",
    "In order to recover the largest generalised eigenvalue and the corresponding eigenvector, one can note that\n",
    "\n",
    "$$Av = \\lambda Bv$$\n",
    "\n",
    "is equivalent to\n",
    "\n",
    "$$B^{-1}Av = \\lambda v.$$\n",
    "\n",
    "Therefore, one has to use the power method to $B^{-1}A$ with a random initialisation. \n",
    "However, it may be rather expensive to invert $B$ explicitly.\n",
    "Therefore, we would like to avoid doing that and inexactly apply $B^{-1}$ through the solving linear system $Bx = y$.\n",
    "Implement the resulting algorithm based on this idea, verify its convergence speed and check that the generalised eigenvalue relation holds.\n",
    "\n",
    "Important notes that will help you successfully update the classical power method:\n",
    " \n",
    "1) think about how the normalization step should be changed, the energy norm definition $\\|x\\|^2_A = x^\\top A x$ can help in the proper reformultation\n",
    "\n",
    "2) think how the step $x_{k+1}= Ax_k$ will be updated with the aforementioned notes regarding operations with matrix $B$ \n",
    "\n",
    "### From *block* power method to its generalisation form\n",
    "\n",
    "Just as in the regular eigenvalue case, generalised eigenvalues can be extended to block power method (which can yield the QZ algoritm, analogously to emergence of QR in the regular case). \n",
    "The extension is straightforward and consists of replacing vectors with matrices corresponding to the $k$ largest  generalised eigenvectors. \n",
    "Moreover, since orthogonality is reqiored, a routine for enforcing orthogonality of the iterates is also needed. Implement the block power method for the generalised eigenvalue problem. \n",
    "Pay special attention how the orthogonalisation procedure shown be changed to take into account the matrix $B$.\n",
    "Separately implement the proper orthoghonalisation algorithm and use it in your code for the target block power method.\n",
    "Test convergence speed and check that results satisfy the target equation.\n",
    "\n",
    "In addition, you can check the correctness of your result with ```scipy.linalg.eigh``` function with proper argumets.\n",
    "\n",
    "### Application of the generalised eignevalue problem to the canonical correlation analysis\n",
    "\n",
    "When one studies two datasets, it may be desirable to find the orthogonal (in the sense of the inner products induced by the correlation matrices) bases of spaces that capture linear combinations of features that are most correlated between the two datasets. \n",
    "Such a problem is called the problem of **canonical correlation analysis** and can be formulated as finding such a sequence $\\{(\\phi_i, \\psi_i)\\}$ that for data matrices $X\\in\\mathbb{R}^{n\\times a}, Y\\in\\mathbb{R}^{n\\times b}$\n",
    "\n",
    "$$(\\phi_i, \\psi_i)\\in \\arg\\max_{\\phi, \\psi} \\phi^TS_{xy}\\psi$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\\|\\phi\\|_{S_x} = \\|\\psi\\|_{S_y} = 1$$\n",
    "\n",
    "$$\\langle\\phi, \\phi_j\\rangle_{S_x} = 0 \\quad \\forall j<i$$\n",
    "\n",
    "\n",
    "$$\\langle\\psi, \\psi_j\\rangle_{S_y} = 0 \\quad \\forall j<i,$$\n",
    "\n",
    "where $\\|\\cdot\\|_A$ is the energy norm induced by $A$, $\\langle\\cdot,\\cdot\\rangle_A$ is the inner product induced by $A$, $S_y = \\frac{1}{n}Y^\\top Y$, $S_x = \\frac{1}{n}X^\\top X$, $S_{xy} = \\frac{1}{n}X^\\top Y$\n",
    "\n",
    "We are asked to find two bases of the maximal-cross-correlation subspace, both orthogonal in the inner product induced by the correlation matrix of the appropriate dataset.\n",
    "\n",
    "Consider firstly the problem of simply finding a basis of the described space, subject only to unit-vector conditions (since orthogonalisation can always be carried out separately). \n",
    "The Lagrangian will then read:\n",
    "\n",
    "$$L(\\phi, \\psi, \\lambda, \\mu) = \\phi^TS_{xy}\\psi-\\lambda(\\phi^TS_x\\phi-1)-\\mu(\\psi^TS_y\\psi-1),$$\n",
    "\n",
    "where $\\lambda$ and $\\mu$ are dual variables.\n",
    "\n",
    "From the stationarity of Lagrangian with respect ot the original variables $\\psi$ and $\\phi$ follows the following equation on the dual varibles\n",
    "\n",
    "$$S_{xy}\\psi = \\lambda S_x\\phi$$\n",
    "\n",
    "$$S_{yx}\\phi = \\mu S_y\\psi$$\n",
    "\n",
    "Note that\n",
    "\n",
    "$$\\|\\phi\\|_{S_x} = \\phi^TS_x\\phi = \\frac{1}{\\lambda}\\phi^T S_{xy}\\psi = \\frac{1}{\\lambda} \\psi^TS_{yx}\\phi = \\frac{\\mu}{\\lambda}\\psi^TS_y\\psi = \\frac{\\mu}{\\lambda}\\|\\psi\\|_{S_y} = \\frac{\\mu}{\\lambda} = 1,$$\n",
    "\n",
    "Therefore $\\mu=\\lambda$ and the equations above can be simplified in the following way\n",
    "\n",
    "$$S_{xy}\\psi = \\lambda S_x\\phi$$\n",
    "\n",
    "$$S_{yx}\\phi = \\lambda S_y\\psi$$\n",
    "\n",
    "and the problem admits a block representation:\n",
    "\n",
    "$$\\begin{bmatrix}0 & S_{xy} \\\\ S_{yx} & 0\\end{bmatrix}\\begin{bmatrix}\\phi\\\\\\psi\\end{bmatrix} = \\lambda \\begin{bmatrix}S_x & 0 \\\\ 0 & S_y\\end{bmatrix}\\begin{bmatrix}\\phi\\\\\\psi\\end{bmatrix}. $$\n",
    "\n",
    "Surprisingly, this problem has the form of the generalised eigenvalue problem. \n",
    "Therefore, the target vectors can be found as the leading eigenvectors and will be an orthonormal basis.\n",
    "\n",
    "You need to implement the algorithm for extracting $k$ vectors $\\psi_i, \\phi_i$ from the given data matrices $X \\in \\mathbb{R}^{n \\times d_1}$ and $Y \\in \\mathbb{R}^{n \\times d_2}$.\n",
    "You should use the power method from the previous task and take into account the special structure of the matrices $(A, B)$ coming from the original problem statment. Please, use this structure for efficient implementation of the internal steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f3edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d030ec",
   "metadata": {},
   "source": [
    "- You are further asked to test the developed algorithm on an arbitrary splitting of the MNIST dataset (import it from [keras](https://keras.io/api/datasets/mnist/), [PyTorch](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST), etc). Test different values of $k$.\n",
    "- In addition, test it on synthetic data of known covariance. \n",
    "You can generate bivariate data with known covariance matrix through the multiplication of vectors from standard normal distribution by the Cholesky factor of the desired covariance matrix.\n",
    "Then split the generated bivaritate data into two univariate vectors. Run the prepared code and comment on the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10241bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code is here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
