{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem set 1 (55 pts)\n",
        "\n",
        "## Important information\n",
        "\n",
        "1. We provide signatures of the functions that you have to implement. Make sure you follow the signatures defined, otherwise your coding solutions will not be graded.\n",
        "\n",
        "2. Please submit the single Jupyter Notebook file, where only Python and Markdown/$\\LaTeX$\n",
        " are used. Any hand-written solutions inserted by photos or in any other way are prohibitive and will not be graded. If you will have any questions about using Markdown, ask them!\n",
        "\n",
        "3. The works will be checked for plagiarism. The score will be divided by the number of similar works."
      ],
      "metadata": {
        "id": "qQ3x6VfoEpm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1 (25 pts)\n",
        "\n",
        "Assume we have a set of data points $x^{(i)}\\in\\mathbb{R}^{n},\\,i=1,\\dots,m$, and decide to represent this data as a matrix\n",
        "\n",
        "$$\n",
        "  X =\n",
        "    \\begin{pmatrix}\n",
        "      | & & | \\\\\n",
        "      x^{(1)} & \\dots & x^{(m)} \\\\\n",
        "      | & & | \\\\\n",
        "    \\end{pmatrix} \\in \\mathbb{R}^{n \\times m}.\n",
        "$$\n",
        "\n",
        "We suppose that $\\text{rank}\\,X = r$.\n",
        "\n",
        "In all tasks below, we ask you to find the rank of some matrix $M$ related to $X$.\n",
        "In particular, you need to find relation between $\\text{rank}\\,X = r$ and $\\text{rank}\\,M$, e.g., that the rank of $M$ is always larger/smaller than the rank of $X$ or that $\\text{rank}\\,M = \\text{rank}\\,X \\big / 35$.\n",
        "Please support your answer with legitimate arguments and make the answer as accurate as possible.\n",
        "\n",
        "Note that depending on the structure of the matrix $X$, border cases are possible.\n",
        "Make sure to cover them in your answer correctly.\n",
        "\n",
        "**Task 1.** (5 pts)\n",
        "In applied statistics and machine learning, data is often normalized.\n",
        "One particularly popular strategy is to subtract the estimated mean $\\mu$ and divide by the square root of the estimated variance $\\sigma^2$. i.e.\n",
        " $$x \\rightarrow (x - \\mu) \\big / \\sigma.$$\n",
        "\n",
        "After the normalization, we get a new matrix\n",
        "  \\begin{equation}\n",
        "    \\begin{split}\n",
        "      Y &:=\n",
        "      \\begin{pmatrix}\n",
        "        | & & | \\\\\n",
        "        y^{(1)} & \\dots & y^{(m)} \\\\\n",
        "        | & & | \\\\\n",
        "      \\end{pmatrix},\\\\\n",
        "      y^{(i)} &:= \\frac{x^{(i)} - \\frac{1}{m}\\sum_{i=1}^{m} x^{(i)}}{\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m} \\left(x^{(i)}\\right)^2 - \\left(\\frac{1}{m}\\sum_{i=1}^{m} x^{(i)}\\right)^2}}.\n",
        "    \\end{split}\n",
        "  \\end{equation}\n",
        "  \n",
        "What is the rank of $Y$ if $\\text{rank} \\; X = r$?\n",
        "\n",
        "**Task 2.** (5 pts)\n",
        "To reveal the structure of data one often considers similarity measures such as [cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
        "  \n",
        "  $$\n",
        "    d_{c}\\left(x^{(i)}, x^{(j)}\\right) := 1 - \\frac{\\sum_{k=1}^{n}x^{(i)}_{k} x^{(j)}_{k}}{\\sqrt{\\sum_{k=1}^{n}\\left(x^{(i)}_{k}\\right)^{2}}\\sqrt{\\sum_{k=1}^{n}\\left(x^{(j)}_{k}\\right)^{2}}} = 1 - \\frac{\\left(x^{(i)}\\right)^{\\top} x^{(j)}}{\\left\\|x^{(i)}\\right\\|_{2}\\left\\|x^{(j)}\\right\\|_{2}}.\n",
        "  $$\n",
        "\n",
        "  Consider a matrix with cosine distances $D = [d_{ij}]$ such that\n",
        "  $$\n",
        "    d_{ij} = d_{c}\\left(x^{(i)}, x^{(j)}\\right).\n",
        "  $$\n",
        "\n",
        "  What is the rank of $D$ if $\\text{rank} \\; X = r$?\n",
        "\n",
        "**Task 3.** (15 pts)\n",
        "Transformation in Task 1 has a form $x \\rightarrow a x + b$, where $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}^n$.\n",
        "In this task, we explore more general transformations.\n",
        "\n",
        "- Let $y^{(i)} = A x^{(i)} + b$, where $A\\in \\mathbb{R}^{p\\times n}$, $b \\in \\mathbb{R}^{p}$. What possible ranks matrix\n",
        "  \\begin{equation}\n",
        "    \\begin{split}\n",
        "      Y &:=\n",
        "      \\begin{pmatrix}\n",
        "        | & & | \\\\\n",
        "        y^{(1)} & \\dots & y^{(m)} \\\\\n",
        "        | & & | \\\\\n",
        "      \\end{pmatrix}\n",
        "    \\end{split}\n",
        "  \\end{equation}\n",
        "  may have for different $A$?\n",
        "  \n",
        "  - If $p > n$ is it possible that $\\text{rank}\\,Y > \\text{rank}\\,X$?\n",
        "  \n",
        "  - If $p = n$ and both matrix $A$ and vector $b$ are not identically zeros how small the rank of $Y$ can be?\n",
        "\n",
        "  - How large the rank of $Y$ can be?\n",
        "\n",
        "- Let $y^{(i)} = w \\odot x^{(i)} + b$, where $w, b \\in \\mathbb{R}^{n}$ and $\\odot$ is an [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)).\n",
        "\n",
        "What possible ranks matrix $Y$ may have for different $w$ and $b$?"
      ],
      "metadata": {
        "id": "gy5JtCkXHvhW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES0xdErsEjUe"
      },
      "outputs": [],
      "source": [
        "# Your solution is here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2 (10 pts)\n",
        "\n",
        "- Let $\\| \\cdot \\|$ and $\\| \\cdot \\|'$ be norms on a vector space $X$, and let $B$ and $B'$ denote the corresponding close unit balls.\n",
        "Prove that $B \\subseteq B'$ iff $\\| \\cdot \\|' \\leq \\| \\cdot \\|$ for any input vectors.\n",
        "    \n",
        "Let $1 \\leq p \\leq q \\leq \\infty$:\n",
        "\n",
        "-  Prove that $\\| \\cdot \\|_q \\leq \\| \\cdot \\|_p$ on $\\mathbb{R}^n$\n",
        "\n",
        "- Show that there exists a constant $C = C(n,p,q) > 0$ such that $\\| \\cdot \\|_p \\leq C\\| \\cdot \\|_q$ on $\\mathbb{R}^n$\n",
        "\n",
        "- Can the above constant be chosen in such a way that it does not depend on $n$?\n",
        "\n",
        "- Find the smallest possible $C(n,p,q)$ with the above property."
      ],
      "metadata": {
        "id": "hervgMfqHxOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution is here"
      ],
      "metadata": {
        "id": "XL3khOcXgvj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3: Recover 3D molecule structure via low-rank approximation (20 pts)\n",
        "\n",
        "***Chromatin***\n",
        "\n",
        "Imagine you are given the task of understanding the structure of a complex geometric shape, but there's a catch: you can not observe the shape directly in three dimensions.\n",
        "Instead, you can only infer its structure by studying the points of contact between different parts of the shape.\n",
        "This is analogous to the challenge faced in 3D chromatin modeling, where the goal is to understand the three-dimensional organization of the DNA sequence within a cell's nucleusâ€”a structure too tiny and complex to be directly observed in detail.\n",
        "\n",
        "To tackle this, scientists employ a technique known as Hi-C.\n",
        "The Hi-C technique generates a symmetric matrix such that elements in the matrix quantify the interaction frequencies between pairs of chromatin segments.\n",
        "The higher values indicate a higher propensity for spatial proximity.\n",
        "We consider $N$ segments, therefore the dimension of the resulting matrix is $N \\times N$.\n",
        "\n",
        "Mathematically, the matrix is sparse due to the infrequent interactions between most DNA segments and has a block diagonal structure reflecting clusters of interactions within certain genomic regions.\n",
        "The matrix patterns are analyzed using algorithms and statistical models to infer the three-dimensional chromatin structure.\n",
        "\n",
        "Let us inspect what Hi-C looks like!"
      ],
      "metadata": {
        "id": "vNXNodmpXNnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly\n",
        "!pip install patsy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "590oxg7LYtQ2",
        "outputId": "bd2d3164-73b0-4a81-ce83-8af87353e5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (23.2)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (0.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.4 in /usr/local/lib/python3.10/dist-packages (from patsy) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "from matplotlib import pyplot as plt, cm\n",
        "from matplotlib import colors"
      ],
      "metadata": {
        "id": "dRKhxvHXYtw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'IMR90_100kb_chr20.csv'\n",
        "C = np.array(pd.read_csv(f'{path}', header=None))\n",
        "\n",
        "# Removes the blanks obainted during the experiment\n",
        "mask = (C == 0).all(0)\n",
        "index = np.where(~mask)[0]\n",
        "C = C[~mask,:]\n",
        "C = C[:,~mask]\n",
        "\n",
        "\n",
        "im = plt.imshow(C, cmap=cm.rainbow, norm=colors.LogNorm())\n",
        "plt.colorbar(im)\n",
        "plt.show()\n",
        "\n",
        "print('C shape', C.shape, f'n={C.shape[0]}')"
      ],
      "metadata": {
        "id": "_yDCdj-eYx0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that our goal is to infer the 3D chromatin structure.\n",
        "Mathematically, we model the chromatin as a smooth curve in 3D space.\n",
        "That is, the points lie on a smooth parametrized curve $x_1, \\ldots x_n \\in \\gamma(t)$, where $\\gamma(t)=\\left(\\begin{array}{c}\\gamma_1(t) \\\\ \\gamma_2(t) \\\\ \\gamma_3(t)\\end{array}\\right)$.\n",
        "\n",
        "Each coordinate can be expanded using [cubic spline basis functions](https://en.wikipedia.org/wiki/Spline_interpolation) $h_1(t), \\ldots, h_k(t)$ evaluated at the sequence of coordinates at $t=1, \\ldots, n = 599$  :\n",
        "$$\n",
        "\\gamma_j(t)=\\sum_{\\ell=1}^k \\Theta_{\\ell j} h_{\\ell}(t), \\quad j=1,2,3\n",
        "$$\n",
        "\n",
        "$$\n",
        "x_i=\\gamma(i)=\n",
        "\\begin{pmatrix}\n",
        "\\sum_{\\ell=1}^k \\Theta_{\\ell 1} h_{\\ell}(i) \\\\\n",
        "\\sum_{\\ell=1}^k \\Theta_{\\ell 2} h_{\\ell}(i) \\\\\n",
        "\\sum_{\\ell=1}^k \\Theta_{\\ell 3} h_{\\ell}(i)\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "X=\\begin{pmatrix}\n",
        "-x_1^\\top - \\\\\n",
        "\\cdots \\\\\n",
        "-x_n^\\top-\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "\\mid & \\mid & \\mid \\\\\n",
        "\\gamma_1 & \\gamma_2 & \\gamma_3 \\\\\n",
        "\\mid & \\mid & \\mid\n",
        "\\end{pmatrix} \\in \\mathbb{R}^{n \\times 3}\n",
        "$$\n",
        "\n",
        "Thus, we can rewrite the coordinates of the chromatin stacked in matrix $X$ as product $X=H \\Theta$, where\n",
        "\n",
        "$$\n",
        "H=\\begin{pmatrix}\n",
        "\\mid & & \\mid \\\\\n",
        "h_1 & \\ldots & h_k \\\\\n",
        "\\mid & & \\mid\n",
        "\\end{pmatrix} \\in \\mathbb{R}^{n \\times k}\n",
        "$$\n",
        "\n",
        "The optimization problem that will infer the 3D structure is the following:\n",
        "\n",
        "$$\n",
        "\\ell\\left(x_1, \\ldots, x_n\\right)=\\sum_{i=1}^n \\sum_{j=1}^n\\left(Z_{i j}-\\left\\langle x_i, x_j\\right\\rangle\\right)^2 \\to \\min_{\\Theta \\in \\mathbb{R}^{k \\times 3}}\n",
        "$$\n",
        "\n",
        "or equivalently\n",
        "\n",
        "$$\n",
        " \\min_{\\Theta \\in \\mathbb{R}^{k \\times 3}}\\|Z-S(X)\\|_F^2 = \\min_{\\Theta \\in \\mathbb{R}^{k \\times 3}} \\|Z-S(H \\Theta)\\|_F^2\n",
        "$$\n",
        "\n",
        "where $Z$ is a normalized Hi-C matrix and $S$ is some additional map.\n",
        "\n",
        "The following cell performs the normalization step so you do not have to worry about it."
      ],
      "metadata": {
        "id": "uyIQOb9EY48Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import patsy\n",
        "\n",
        "def scale(y, center=True, scale=True):\n",
        "    x = y.copy()\n",
        "    if center:\n",
        "        x -= x.mean(axis=0)\n",
        "    if scale and center:\n",
        "        x /= x.std()\n",
        "    return x\n",
        "\n",
        "df = 50\n",
        "n_knots = df - 4\n",
        "knots = np.linspace(1, max(index), n_knots, dtype='int64')\n",
        "knots = knots[1:n_knots-1]\n",
        "B0 = patsy.bs(index, df = df, include_intercept=True)\n",
        "H = B0[::]\n",
        "\n",
        "D = 1/(C+1)\n",
        "Z = -D**2/2\n",
        "Z = scale(Z, center = True, scale = False)\n",
        "Z = (scale(Z.T, center = True, scale = False)).T\n",
        "\n",
        "im = plt.imshow(Z, cmap=cm.rainbow, norm=colors.Normalize(vmin=-0.4, vmax=0.4))\n",
        "plt.colorbar(im)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WIKR1LwpY0hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.** (5 pts) Prove the Lemma:\n",
        "\n",
        "If $H \\in \\mathbb{R}^{n \\times k}$ is a matrix with orthogonal columns, i.e. $H^\\top H=I$, and $S(X)=X X^\\top$ then problem\n",
        "\n",
        "$$\n",
        "\\min_{\\Theta \\in \\mathbb{R}^{k \\times 3}} \\|Z-S(H \\Theta)\\|_F^2\n",
        "$$\n",
        "\n",
        "is equivalent to\n",
        "\n",
        "$$\n",
        "\\min_{\\Theta \\in \\mathbb{R}^{k \\times 3}} \\left\\|H^T Z H-\\Theta \\Theta^T\\right\\|_F^2.\n",
        "$$"
      ],
      "metadata": {
        "id": "b1OJfTvabhoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.** Solving the problem\n",
        "\n",
        "$$\n",
        "\\min_{\\Theta \\in \\mathbb{R}^{k \\times 3}} \\left\\|H^\\top Z H-\\Theta \\Theta^\\top\\right\\|_F^2.\n",
        "$$\n",
        "\n",
        "can be interpreted as approximating the matrix $H^\\top Z H$ by a positive semidefinite rank-3 matrix $\\Theta \\Theta^\\top$.\n",
        "\n",
        "Assuming that the symmetric matrix $H^\\top Z H$ has at least $3$ positive eigenvalues the solution can be found via eigendecomposition of $H^\\top Z H$:\n",
        "\n",
        "Let $H^\\top Z H=Q \\Lambda Q^\\top$ for orthogonal $Q$ and diagonal $\\Lambda=\\operatorname{diag}\\left(\\lambda_1, \\ldots, \\lambda_n\\right)$ with $\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_n$, then\n",
        "\n",
        "$$\n",
        "\\Theta=Q \\sqrt{\\Lambda_3} \\text {, where } \\sqrt{\\Lambda_3}=\\operatorname{diag}\\left(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2}, \\sqrt{\\lambda_3}, \\ldots, 0, 0\\right) .\n",
        "$$\n",
        "\n",
        "The computational efficiency of the approach derives from the fact that it relies on eigen-decomposition of a small $k \\times k$ matrix, requiring only $O\\left(k^3\\right)$ additional operations.\n",
        "\n",
        "In this task, we will write a function that finds the optimal 3D chromatin representation for the given Hi-C matrix.\n",
        "We do that by optimizing the functional from task 1.\n",
        "\n",
        "\n",
        "\n",
        "- (5 pts) If you recall, the lemma from task 1 requires $H$ to have orthogonal columns.\n",
        "Write a code to verify whether it is true. If it is not, propose and compare the ways to ortogonalize it.\n",
        "\n",
        "\n",
        "- (10 pts) You are given below a template for function ```low_rank_approximation```.\n",
        "Complete the code for minimizing  $\\tilde{\\ell}(\\Theta)$ using the proposed low rank approach.\n",
        "Remember that we model chromatin as 3D curve so the rank of our decomposition should be 3.\n",
        "However, compare it for different ranks: plot the loss value against the ranks (small, medium, large) and plot the reconstructed matrix.\n",
        "Which rank would be a good fit?"
      ],
      "metadata": {
        "id": "MCZJsSIjd6bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_ = lambda X, Z: np.mean((Z - X @ X.T)**2)\n",
        "def low_rank_approximation(Z, H, rank=3):\n",
        "    '''\n",
        "    Solves the minimization problem from task 1\n",
        "\n",
        "    - Z: normalized Hi-C counts matrix\n",
        "    - H: orthogonal matrix\n",
        "    - k: the rank of the eigendecomposition\n",
        "\n",
        "    returns:\n",
        "    - Theta: optimal Theta\n",
        "    - loss: calcluated using ```loss_```\n",
        "    - adj_rank: the rank adjusted for numerical stability. Usage: adj_rank = np.sum(eigen_values > 1e-12)\n",
        "    '''\n",
        "      ### your code is here\n",
        "\n",
        "    X = H @ Theta\n",
        "    return {'Theta': Theta,\n",
        "    'X': X,\n",
        "    'loss': loss_(X, Z),\n",
        "    'adj_rank': rank}\n",
        "\n",
        "Theta, X, loss, rank = low_rank_approximation(Z, H, rank=3).values()\n",
        "Z_hat = X @ X.T\n",
        "print('The optimal loss value is', loss)\n",
        "\n",
        "\n",
        "im = plt.imshow(Z_hat, cmap=cm.rainbow, norm=colors.Normalize(vmin=-0.2, vmax=0.4))\n",
        "plt.colorbar(im)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zoi-SyUbeu2T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}